{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parkinson's ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maya-Poghosyan/Parkinsons-Diagnosis/blob/main/Parkinson's_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7I1Vxc_m5G6"
      },
      "source": [
        "# Machine Learning for Parkinson's Diagnosis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8bekqpetQCQ"
      },
      "source": [
        "\r\n",
        "---\r\n",
        "$Project$ $Goal$: Algorithm that can predict impact of Parkinson's disease on motor and speech functions. <br>\r\n",
        "Dataset info can be found on Github <a href = \"https://github.com/imadtoubal/Parkinson-s-Disease-Classification-from-Speech-Data\">here</a>.<br>\r\n",
        "Note: `class` is a binary identifier of Parkinson's (1 - diseased, 0 - healthy control)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvZkaX3Hn_Mj",
        "outputId": "b19166e1-5da0-4dcc-9dce-4ed357dc3156"
      },
      "source": [
        "# import Google Drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "\r\n",
        "# import libraries (there's a lot)\r\n",
        "import pandas as pd\r\n",
        "import sklearn\r\n",
        "import seaborn as sns\r\n",
        "import scipy\r\n",
        "import numpy as np\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.decomposition import PCA\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn import linear_model\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "import torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "oZFmXUPZm2Qv",
        "outputId": "d05ad8a1-f905-413d-e03b-e5001e1133fc"
      },
      "source": [
        "df = pd.read_csv('/content/gdrive/My Drive/Programming/ACSEF 2021/pd_speech_features.csv')\r\n",
        "df.dropna(axis=0,inplace=True)\r\n",
        "df.groupby('class').mean()\r\n",
        "labels = df['class']\r\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>PPE</th>\n",
              "      <th>DFA</th>\n",
              "      <th>RPDE</th>\n",
              "      <th>numPulses</th>\n",
              "      <th>numPeriodsPulses</th>\n",
              "      <th>meanPeriodPulses</th>\n",
              "      <th>stdDevPeriodPulses</th>\n",
              "      <th>locPctJitter</th>\n",
              "      <th>locAbsJitter</th>\n",
              "      <th>rapJitter</th>\n",
              "      <th>ppq5Jitter</th>\n",
              "      <th>ddpJitter</th>\n",
              "      <th>locShimmer</th>\n",
              "      <th>locDbShimmer</th>\n",
              "      <th>apq3Shimmer</th>\n",
              "      <th>apq5Shimmer</th>\n",
              "      <th>apq11Shimmer</th>\n",
              "      <th>ddaShimmer</th>\n",
              "      <th>meanAutoCorrHarmonicity</th>\n",
              "      <th>meanNoiseToHarmHarmonicity</th>\n",
              "      <th>meanHarmToNoiseHarmonicity</th>\n",
              "      <th>minIntensity</th>\n",
              "      <th>maxIntensity</th>\n",
              "      <th>meanIntensity</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>b1</th>\n",
              "      <th>b2</th>\n",
              "      <th>b3</th>\n",
              "      <th>b4</th>\n",
              "      <th>GQ_prc5_95</th>\n",
              "      <th>GQ_std_cycle_open</th>\n",
              "      <th>GQ_std_cycle_closed</th>\n",
              "      <th>GNE_mean</th>\n",
              "      <th>GNE_std</th>\n",
              "      <th>GNE_SNR_TKEO</th>\n",
              "      <th>...</th>\n",
              "      <th>tqwt_skewnessValue_dec_34</th>\n",
              "      <th>tqwt_skewnessValue_dec_35</th>\n",
              "      <th>tqwt_skewnessValue_dec_36</th>\n",
              "      <th>tqwt_kurtosisValue_dec_1</th>\n",
              "      <th>tqwt_kurtosisValue_dec_2</th>\n",
              "      <th>tqwt_kurtosisValue_dec_3</th>\n",
              "      <th>tqwt_kurtosisValue_dec_4</th>\n",
              "      <th>tqwt_kurtosisValue_dec_5</th>\n",
              "      <th>tqwt_kurtosisValue_dec_6</th>\n",
              "      <th>tqwt_kurtosisValue_dec_7</th>\n",
              "      <th>tqwt_kurtosisValue_dec_8</th>\n",
              "      <th>tqwt_kurtosisValue_dec_9</th>\n",
              "      <th>tqwt_kurtosisValue_dec_10</th>\n",
              "      <th>tqwt_kurtosisValue_dec_11</th>\n",
              "      <th>tqwt_kurtosisValue_dec_12</th>\n",
              "      <th>tqwt_kurtosisValue_dec_13</th>\n",
              "      <th>tqwt_kurtosisValue_dec_14</th>\n",
              "      <th>tqwt_kurtosisValue_dec_15</th>\n",
              "      <th>tqwt_kurtosisValue_dec_16</th>\n",
              "      <th>tqwt_kurtosisValue_dec_17</th>\n",
              "      <th>tqwt_kurtosisValue_dec_18</th>\n",
              "      <th>tqwt_kurtosisValue_dec_19</th>\n",
              "      <th>tqwt_kurtosisValue_dec_20</th>\n",
              "      <th>tqwt_kurtosisValue_dec_21</th>\n",
              "      <th>tqwt_kurtosisValue_dec_22</th>\n",
              "      <th>tqwt_kurtosisValue_dec_23</th>\n",
              "      <th>tqwt_kurtosisValue_dec_24</th>\n",
              "      <th>tqwt_kurtosisValue_dec_25</th>\n",
              "      <th>tqwt_kurtosisValue_dec_26</th>\n",
              "      <th>tqwt_kurtosisValue_dec_27</th>\n",
              "      <th>tqwt_kurtosisValue_dec_28</th>\n",
              "      <th>tqwt_kurtosisValue_dec_29</th>\n",
              "      <th>tqwt_kurtosisValue_dec_30</th>\n",
              "      <th>tqwt_kurtosisValue_dec_31</th>\n",
              "      <th>tqwt_kurtosisValue_dec_32</th>\n",
              "      <th>tqwt_kurtosisValue_dec_33</th>\n",
              "      <th>tqwt_kurtosisValue_dec_34</th>\n",
              "      <th>tqwt_kurtosisValue_dec_35</th>\n",
              "      <th>tqwt_kurtosisValue_dec_36</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.85247</td>\n",
              "      <td>0.71826</td>\n",
              "      <td>0.57227</td>\n",
              "      <td>240</td>\n",
              "      <td>239</td>\n",
              "      <td>0.008064</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.00218</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.00067</td>\n",
              "      <td>0.00129</td>\n",
              "      <td>0.00200</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.517</td>\n",
              "      <td>0.03011</td>\n",
              "      <td>0.03496</td>\n",
              "      <td>0.04828</td>\n",
              "      <td>0.09034</td>\n",
              "      <td>0.970805</td>\n",
              "      <td>0.036223</td>\n",
              "      <td>18.995</td>\n",
              "      <td>69.997496</td>\n",
              "      <td>76.088046</td>\n",
              "      <td>72.465512</td>\n",
              "      <td>539.342735</td>\n",
              "      <td>1031.849040</td>\n",
              "      <td>2447.162183</td>\n",
              "      <td>3655.054806</td>\n",
              "      <td>101.092218</td>\n",
              "      <td>83.147440</td>\n",
              "      <td>255.214830</td>\n",
              "      <td>396.643631</td>\n",
              "      <td>0.77778</td>\n",
              "      <td>11.7245</td>\n",
              "      <td>2.8277</td>\n",
              "      <td>1.17300</td>\n",
              "      <td>0.26512</td>\n",
              "      <td>0.083127</td>\n",
              "      <td>...</td>\n",
              "      <td>0.071728</td>\n",
              "      <td>0.010352</td>\n",
              "      <td>-2.73030</td>\n",
              "      <td>66.5007</td>\n",
              "      <td>36.9934</td>\n",
              "      <td>26.3508</td>\n",
              "      <td>51.9577</td>\n",
              "      <td>21.5451</td>\n",
              "      <td>8.2488</td>\n",
              "      <td>8.0024</td>\n",
              "      <td>6.9635</td>\n",
              "      <td>6.9189</td>\n",
              "      <td>5.0622</td>\n",
              "      <td>4.6068</td>\n",
              "      <td>7.7218</td>\n",
              "      <td>2.7224</td>\n",
              "      <td>2.4171</td>\n",
              "      <td>2.9383</td>\n",
              "      <td>4.2077</td>\n",
              "      <td>3.1541</td>\n",
              "      <td>2.8531</td>\n",
              "      <td>2.7496</td>\n",
              "      <td>2.1550</td>\n",
              "      <td>2.9457</td>\n",
              "      <td>2.1993</td>\n",
              "      <td>1.9830</td>\n",
              "      <td>1.8314</td>\n",
              "      <td>2.0062</td>\n",
              "      <td>1.6058</td>\n",
              "      <td>1.5466</td>\n",
              "      <td>1.5620</td>\n",
              "      <td>2.6445</td>\n",
              "      <td>3.8686</td>\n",
              "      <td>4.2105</td>\n",
              "      <td>5.1221</td>\n",
              "      <td>4.4625</td>\n",
              "      <td>2.6202</td>\n",
              "      <td>3.0004</td>\n",
              "      <td>18.9405</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.76686</td>\n",
              "      <td>0.69481</td>\n",
              "      <td>0.53966</td>\n",
              "      <td>234</td>\n",
              "      <td>233</td>\n",
              "      <td>0.008258</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.00195</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.00052</td>\n",
              "      <td>0.00112</td>\n",
              "      <td>0.00157</td>\n",
              "      <td>0.05516</td>\n",
              "      <td>0.502</td>\n",
              "      <td>0.02320</td>\n",
              "      <td>0.03675</td>\n",
              "      <td>0.06195</td>\n",
              "      <td>0.06961</td>\n",
              "      <td>0.984322</td>\n",
              "      <td>0.017974</td>\n",
              "      <td>21.497</td>\n",
              "      <td>67.415903</td>\n",
              "      <td>73.046374</td>\n",
              "      <td>71.528945</td>\n",
              "      <td>564.363614</td>\n",
              "      <td>1016.367294</td>\n",
              "      <td>2383.565201</td>\n",
              "      <td>3498.681572</td>\n",
              "      <td>58.465428</td>\n",
              "      <td>86.487292</td>\n",
              "      <td>248.357127</td>\n",
              "      <td>218.229722</td>\n",
              "      <td>0.81250</td>\n",
              "      <td>13.8284</td>\n",
              "      <td>2.8908</td>\n",
              "      <td>1.02210</td>\n",
              "      <td>0.22004</td>\n",
              "      <td>0.127410</td>\n",
              "      <td>...</td>\n",
              "      <td>0.729330</td>\n",
              "      <td>0.780410</td>\n",
              "      <td>5.22940</td>\n",
              "      <td>8643.9860</td>\n",
              "      <td>3962.0554</td>\n",
              "      <td>2976.2411</td>\n",
              "      <td>4329.0607</td>\n",
              "      <td>4005.1329</td>\n",
              "      <td>1127.6762</td>\n",
              "      <td>116.5331</td>\n",
              "      <td>20.2332</td>\n",
              "      <td>13.6395</td>\n",
              "      <td>12.5514</td>\n",
              "      <td>13.6965</td>\n",
              "      <td>60.8974</td>\n",
              "      <td>6.2958</td>\n",
              "      <td>3.0323</td>\n",
              "      <td>3.4039</td>\n",
              "      <td>4.3794</td>\n",
              "      <td>2.4329</td>\n",
              "      <td>2.0585</td>\n",
              "      <td>2.1839</td>\n",
              "      <td>2.2061</td>\n",
              "      <td>3.0999</td>\n",
              "      <td>1.9824</td>\n",
              "      <td>1.6227</td>\n",
              "      <td>1.5783</td>\n",
              "      <td>2.0470</td>\n",
              "      <td>1.5772</td>\n",
              "      <td>1.5530</td>\n",
              "      <td>1.5589</td>\n",
              "      <td>3.6107</td>\n",
              "      <td>23.5155</td>\n",
              "      <td>14.1962</td>\n",
              "      <td>11.0261</td>\n",
              "      <td>9.5082</td>\n",
              "      <td>6.5245</td>\n",
              "      <td>6.3431</td>\n",
              "      <td>45.1780</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.85083</td>\n",
              "      <td>0.67604</td>\n",
              "      <td>0.58982</td>\n",
              "      <td>232</td>\n",
              "      <td>231</td>\n",
              "      <td>0.008340</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.00176</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.00057</td>\n",
              "      <td>0.00111</td>\n",
              "      <td>0.00171</td>\n",
              "      <td>0.09902</td>\n",
              "      <td>0.897</td>\n",
              "      <td>0.05094</td>\n",
              "      <td>0.06497</td>\n",
              "      <td>0.07772</td>\n",
              "      <td>0.15282</td>\n",
              "      <td>0.974846</td>\n",
              "      <td>0.026313</td>\n",
              "      <td>17.651</td>\n",
              "      <td>62.661706</td>\n",
              "      <td>71.633549</td>\n",
              "      <td>68.086583</td>\n",
              "      <td>548.444604</td>\n",
              "      <td>1032.406341</td>\n",
              "      <td>2357.826954</td>\n",
              "      <td>3678.128717</td>\n",
              "      <td>160.387771</td>\n",
              "      <td>54.685168</td>\n",
              "      <td>151.694847</td>\n",
              "      <td>84.240339</td>\n",
              "      <td>0.81818</td>\n",
              "      <td>26.9273</td>\n",
              "      <td>2.6975</td>\n",
              "      <td>0.84951</td>\n",
              "      <td>0.15756</td>\n",
              "      <td>0.116890</td>\n",
              "      <td>...</td>\n",
              "      <td>0.269510</td>\n",
              "      <td>-0.005522</td>\n",
              "      <td>0.35054</td>\n",
              "      <td>29.2717</td>\n",
              "      <td>32.4971</td>\n",
              "      <td>38.9453</td>\n",
              "      <td>85.1480</td>\n",
              "      <td>62.5132</td>\n",
              "      <td>9.7308</td>\n",
              "      <td>6.8890</td>\n",
              "      <td>6.1357</td>\n",
              "      <td>5.4926</td>\n",
              "      <td>3.9078</td>\n",
              "      <td>4.0864</td>\n",
              "      <td>6.9828</td>\n",
              "      <td>3.4411</td>\n",
              "      <td>3.5173</td>\n",
              "      <td>3.7204</td>\n",
              "      <td>4.2212</td>\n",
              "      <td>3.4881</td>\n",
              "      <td>3.4851</td>\n",
              "      <td>3.3007</td>\n",
              "      <td>2.0427</td>\n",
              "      <td>3.1436</td>\n",
              "      <td>2.1203</td>\n",
              "      <td>1.6627</td>\n",
              "      <td>1.6731</td>\n",
              "      <td>3.2597</td>\n",
              "      <td>1.5921</td>\n",
              "      <td>1.5399</td>\n",
              "      <td>1.5643</td>\n",
              "      <td>2.3308</td>\n",
              "      <td>9.4959</td>\n",
              "      <td>10.7458</td>\n",
              "      <td>11.0177</td>\n",
              "      <td>4.8066</td>\n",
              "      <td>2.9199</td>\n",
              "      <td>3.1495</td>\n",
              "      <td>4.7666</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.41121</td>\n",
              "      <td>0.79672</td>\n",
              "      <td>0.59257</td>\n",
              "      <td>178</td>\n",
              "      <td>177</td>\n",
              "      <td>0.010858</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.00419</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.00149</td>\n",
              "      <td>0.00268</td>\n",
              "      <td>0.00446</td>\n",
              "      <td>0.05451</td>\n",
              "      <td>0.527</td>\n",
              "      <td>0.02395</td>\n",
              "      <td>0.02857</td>\n",
              "      <td>0.04462</td>\n",
              "      <td>0.07185</td>\n",
              "      <td>0.968343</td>\n",
              "      <td>0.042003</td>\n",
              "      <td>19.865</td>\n",
              "      <td>76.306989</td>\n",
              "      <td>81.000749</td>\n",
              "      <td>79.190593</td>\n",
              "      <td>819.529588</td>\n",
              "      <td>1201.813897</td>\n",
              "      <td>3154.035654</td>\n",
              "      <td>4122.163933</td>\n",
              "      <td>238.667052</td>\n",
              "      <td>191.984916</td>\n",
              "      <td>573.752909</td>\n",
              "      <td>526.147599</td>\n",
              "      <td>0.98548</td>\n",
              "      <td>139.5744</td>\n",
              "      <td>1.6961</td>\n",
              "      <td>0.83405</td>\n",
              "      <td>0.17295</td>\n",
              "      <td>0.147370</td>\n",
              "      <td>...</td>\n",
              "      <td>0.366920</td>\n",
              "      <td>-0.492650</td>\n",
              "      <td>0.19164</td>\n",
              "      <td>591.0116</td>\n",
              "      <td>65.3827</td>\n",
              "      <td>53.9852</td>\n",
              "      <td>45.4458</td>\n",
              "      <td>34.6650</td>\n",
              "      <td>88.3259</td>\n",
              "      <td>46.6869</td>\n",
              "      <td>23.2911</td>\n",
              "      <td>19.2022</td>\n",
              "      <td>42.3957</td>\n",
              "      <td>100.3649</td>\n",
              "      <td>7.1967</td>\n",
              "      <td>6.3892</td>\n",
              "      <td>6.5496</td>\n",
              "      <td>6.0264</td>\n",
              "      <td>4.7656</td>\n",
              "      <td>4.8909</td>\n",
              "      <td>4.2531</td>\n",
              "      <td>3.0295</td>\n",
              "      <td>2.0362</td>\n",
              "      <td>1.8478</td>\n",
              "      <td>2.5776</td>\n",
              "      <td>2.2064</td>\n",
              "      <td>1.9491</td>\n",
              "      <td>1.9120</td>\n",
              "      <td>1.8829</td>\n",
              "      <td>6.9761</td>\n",
              "      <td>3.7805</td>\n",
              "      <td>3.5664</td>\n",
              "      <td>5.2558</td>\n",
              "      <td>14.0403</td>\n",
              "      <td>4.2235</td>\n",
              "      <td>4.6857</td>\n",
              "      <td>4.8460</td>\n",
              "      <td>6.2650</td>\n",
              "      <td>4.0603</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.32790</td>\n",
              "      <td>0.79782</td>\n",
              "      <td>0.53028</td>\n",
              "      <td>236</td>\n",
              "      <td>235</td>\n",
              "      <td>0.008162</td>\n",
              "      <td>0.002669</td>\n",
              "      <td>0.00535</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.00166</td>\n",
              "      <td>0.00227</td>\n",
              "      <td>0.00499</td>\n",
              "      <td>0.05610</td>\n",
              "      <td>0.497</td>\n",
              "      <td>0.02909</td>\n",
              "      <td>0.03327</td>\n",
              "      <td>0.05278</td>\n",
              "      <td>0.08728</td>\n",
              "      <td>0.975754</td>\n",
              "      <td>0.027139</td>\n",
              "      <td>19.557</td>\n",
              "      <td>76.645686</td>\n",
              "      <td>80.937258</td>\n",
              "      <td>79.183495</td>\n",
              "      <td>846.796144</td>\n",
              "      <td>1215.346469</td>\n",
              "      <td>3201.513132</td>\n",
              "      <td>4085.456839</td>\n",
              "      <td>402.216738</td>\n",
              "      <td>210.061394</td>\n",
              "      <td>203.637106</td>\n",
              "      <td>384.611697</td>\n",
              "      <td>0.97847</td>\n",
              "      <td>102.0549</td>\n",
              "      <td>15.4045</td>\n",
              "      <td>0.83556</td>\n",
              "      <td>0.16210</td>\n",
              "      <td>0.151990</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.378360</td>\n",
              "      <td>-0.035805</td>\n",
              "      <td>0.26596</td>\n",
              "      <td>32.0393</td>\n",
              "      <td>7.2343</td>\n",
              "      <td>6.5540</td>\n",
              "      <td>6.4520</td>\n",
              "      <td>6.9274</td>\n",
              "      <td>10.2265</td>\n",
              "      <td>14.2579</td>\n",
              "      <td>10.6181</td>\n",
              "      <td>8.6143</td>\n",
              "      <td>9.5822</td>\n",
              "      <td>34.0835</td>\n",
              "      <td>4.1139</td>\n",
              "      <td>3.6718</td>\n",
              "      <td>4.4360</td>\n",
              "      <td>4.5793</td>\n",
              "      <td>3.9447</td>\n",
              "      <td>4.1253</td>\n",
              "      <td>3.4869</td>\n",
              "      <td>2.4627</td>\n",
              "      <td>2.1073</td>\n",
              "      <td>1.9056</td>\n",
              "      <td>2.2214</td>\n",
              "      <td>2.0588</td>\n",
              "      <td>1.8157</td>\n",
              "      <td>1.7577</td>\n",
              "      <td>1.8821</td>\n",
              "      <td>7.8832</td>\n",
              "      <td>6.1727</td>\n",
              "      <td>5.8416</td>\n",
              "      <td>6.0805</td>\n",
              "      <td>5.7621</td>\n",
              "      <td>7.7817</td>\n",
              "      <td>11.6891</td>\n",
              "      <td>8.2103</td>\n",
              "      <td>5.0559</td>\n",
              "      <td>6.1164</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 755 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  gender  ...  tqwt_kurtosisValue_dec_36  class\n",
              "0   0       1  ...                    18.9405      1\n",
              "1   0       1  ...                    45.1780      1\n",
              "2   0       1  ...                     4.7666      1\n",
              "3   1       0  ...                     4.0603      1\n",
              "4   1       0  ...                     6.1164      1\n",
              "\n",
              "[5 rows x 755 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1PsZu2ZYLMK"
      },
      "source": [
        "Here is a simple function that can test metrics of our models as we go along -- it checks for accuracy, precision, and recall."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdIBz07qjWeO"
      },
      "source": [
        "# model stats calculator (can this work with CNNS?) \r\n",
        "def model_stats(y_test,y_pred):\r\n",
        "  accuracy = metrics.accuracy_score(y_test, y_pred)\r\n",
        "  precision = metrics.precision_score(y_test, y_pred)\r\n",
        "  recall = metrics.recall_score(y_test, y_pred)\r\n",
        "  print(accuracy)\r\n",
        "  print(precision)\r\n",
        "  print(recall)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meiEm8Xqihwe"
      },
      "source": [
        "# Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAiK0NOXm-Zh"
      },
      "source": [
        "**Principal Component Analysis** - \"scales down\" data and dimensions to find most important correlations and relationships between the data. More information can be found <a href = \"https://www.datacamp.com/community/tutorials/principal-component-analysis-in-python\">here</a>. Standard imports are as follows: <br>\r\n",
        "`from sklearn.decomposition import PCA`<br>\r\n",
        "`from sklearn.preprocessing import StandardScaler`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xq4SR7rfbFN"
      },
      "source": [
        "df2 = df.drop(columns='class',axis=1)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30BYPudDw9Jg",
        "outputId": "7ac42917-6180-43d3-cb05-1dc4e3c43512"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "scaler1 = MinMaxScaler()\r\n",
        "scaler1.fit(df2)\r\n",
        "feature_scaled = scaler1.transform(df2)\r\n",
        "pc1 = PCA(n_components=4)\r\n",
        "pc1.fit(feature_scaled)\r\n",
        "feature_scaled_pca = pc1.transform(feature_scaled)\r\n",
        "print(np.shape(feature_scaled_pca))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(756, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMiP5kkdzdwO",
        "outputId": "8efd4445-1b29-4cd0-9bc1-2e9530c249e1"
      },
      "source": [
        "pcomponent1 = feature_scaled_pca[:, 0]\r\n",
        "pcomponent2 = feature_scaled_pca[:, 1]\r\n",
        "pcomponent3 = feature_scaled_pca[:, 2]\r\n",
        "pcomponent4 = feature_scaled_pca[:, 3]\r\n",
        "feat_var = np.var(feature_scaled_pca, axis=0)\r\n",
        "feat_var_rat = feat_var/(np.sum(feat_var))\r\n",
        "print(\"Variance Ratio of the 4 Principal Components Analysis: \", feat_var_rat)\r\n",
        "print(\"Our four principal components account for almost 90% of variance in the dataset.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variance Ratio of the 4 Principal Components Analysis:  [0.42260888 0.28561276 0.1596861  0.13209226]\n",
            "Our four principal components account for almost 90% of variance in the dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "MdqMOi2WcOsd",
        "outputId": "a670bccb-4caa-40fd-ea7b-ca7a7f73c01b"
      },
      "source": [
        "df = pd.DataFrame(feature_scaled_pca, columns = ['principal component 1', 'principal component 2',\r\n",
        "                                                 'principal component 3', 'principal component 4'])\r\n",
        "df['label'] = labels\r\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>principal component 1</th>\n",
              "      <th>principal component 2</th>\n",
              "      <th>principal component 3</th>\n",
              "      <th>principal component 4</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.345768</td>\n",
              "      <td>-0.561464</td>\n",
              "      <td>-1.012609</td>\n",
              "      <td>-0.137229</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.457105</td>\n",
              "      <td>-0.497098</td>\n",
              "      <td>-1.117513</td>\n",
              "      <td>0.188325</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.929426</td>\n",
              "      <td>-0.180870</td>\n",
              "      <td>-0.951920</td>\n",
              "      <td>0.269965</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.386014</td>\n",
              "      <td>-1.766024</td>\n",
              "      <td>1.813981</td>\n",
              "      <td>-0.398953</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.161909</td>\n",
              "      <td>-1.189702</td>\n",
              "      <td>1.912959</td>\n",
              "      <td>-0.745900</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   principal component 1  principal component 2  ...  principal component 4  label\n",
              "0              -1.345768              -0.561464  ...              -0.137229      1\n",
              "1              -1.457105              -0.497098  ...               0.188325      1\n",
              "2              -1.929426              -0.180870  ...               0.269965      1\n",
              "3              -0.386014              -1.766024  ...              -0.398953      1\n",
              "4              -0.161909              -1.189702  ...              -0.745900      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaakayXWg1UW",
        "outputId": "b036fbfa-93f8-409d-b6f9-dbbea0903135"
      },
      "source": [
        "odata = pd.read_csv('/content/gdrive/My Drive/Programming/ACSEF 2021/pd_speech_features.csv')\r\n",
        "odata.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(756, 755)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vta5NpNdevjq",
        "outputId": "2423e182-60c3-4675-8a61-d16040108a10"
      },
      "source": [
        "''' This loop gives us the top feature of each component using the argmax function '''\r\n",
        "top_features = [np.abs(pc1.components_[i]).argmax() for i in range(pc1.components_.shape[0])]\r\n",
        "\r\n",
        "\r\n",
        "''' Now, we go back to our original feature names (columns of our dataset), and get the names of the 4 key features '''\r\n",
        "top_feature_names = [list(odata.columns)[top_features[i]] for i in range(pc1.components_.shape[0])]\r\n",
        "\r\n",
        "''' printing the names '''\r\n",
        "for n,name in enumerate(top_feature_names):\r\n",
        "\tprint(\"#\" + str(n) + \": \" + str(name))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#0: tqwt_entropy_shannon_dec_16\n",
            "#1: gender\n",
            "#2: det_entropy_log_7_coef\n",
            "#3: gender\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmaxECgo1BYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee26095e-8f23-42b6-ec69-d134402552a2"
      },
      "source": [
        "# SHAPIRO - should be lower than 0.005\r\n",
        "f, p = scipy.stats.f_oneway(df['principal component 1'][df['label'] == 0],\r\n",
        "               df['principal component 1'][df['label'] == 1])\r\n",
        "print(f'Our p-value is {p}.')\r\n",
        "print(\"This indicates real correlation in the data.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our p-value is 2.4100254916016902e-36.\n",
            "This indicates real correlation in the data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG-pzlVKimOc"
      },
      "source": [
        "# Simpler Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnN_ZrHUaFH1"
      },
      "source": [
        "**Logistic Regression** - predicting whether a patient will be 0 (Healthy), or 1 (Parkinson's). More info can be found <a href = \"https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python\">here</a>. Input labels are the principal components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORhX9DERTRlm",
        "outputId": "5ef01393-ca68-425b-c055-decdf7aa6e10"
      },
      "source": [
        "train_df, test_df = train_test_split(df, test_size = 0.4, random_state = 1)\r\n",
        "train_df.head()\r\n",
        "input_labels = ['principal component 1','principal component 2','principal component 3', 'principal component 4']\r\n",
        "output_label = 'label'\r\n",
        "\r\n",
        "x_train = train_df[input_labels]\r\n",
        "y_train = train_df[output_label]\r\n",
        "class_rm = linear_model.LogisticRegression()\r\n",
        "class_rm.fit(x_train,y_train)\r\n",
        "x_test = test_df[input_labels]\r\n",
        "y_test = test_df[output_label].values.squeeze() # what does squeeze mean\r\n",
        "y_pred = class_rm.predict(x_test)\r\n",
        "y_pred = y_pred.squeeze()\r\n",
        "x_test_view = x_test[input_labels].values.squeeze()\r\n",
        "y_prob = class_rm.predict_proba(x_test)\r\n",
        "model_stats(y_test,y_pred)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7887788778877888\n",
            "0.8617886178861789\n",
            "0.8760330578512396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTIuU3T2ysfl"
      },
      "source": [
        "Let's test a single Support Vector Machine (SVM)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IRPhzWcsf1s",
        "outputId": "4ce45a64-d7ee-4f45-ad26-7259cce65b89"
      },
      "source": [
        "from sklearn import svm\r\n",
        "clf = svm.SVC()\r\n",
        "clf.fit(x_train,y_train)\r\n",
        "clfp = clf.predict(x_test)\r\n",
        "model_stats(y_test, clfp)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8151815181518152\n",
            "0.8549618320610687\n",
            "0.9256198347107438\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3ODtIEGyE_I"
      },
      "source": [
        "We can also test a pre-built multilayer perceptron convolutional neural network model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mcvJ0HzvZGQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbd70258-8195-4b4f-e11e-059140b14a99"
      },
      "source": [
        "nnet = MLPClassifier(hidden_layer_sizes=(10), max_iter= 1000)  \r\n",
        "nnet.fit(x_train, y_train)\r\n",
        "predictions = nnet.predict(x_test)\r\n",
        "print(\"MLP Testing Set Score:\")\r\n",
        "print(accuracy_score(y_test, predictions)*100)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP Testing Set Score:\n",
            "82.17821782178217\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X07LwKpUwJx0"
      },
      "source": [
        "# Bagging Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f87qxEU9ZGA_"
      },
      "source": [
        "**B**ootstrap **Agg**regat**ing**, or \"bagging\", is an ensemble method that combines the results of many decision trees and support vector machines. <br> We will also be using the KFolding method, which splits and shuffles our data to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8TUApq0pMC5",
        "outputId": "aef252ab-5ce5-4ebf-d4cf-b4c4f3e050bb"
      },
      "source": [
        "from sklearn.model_selection import KFold\r\n",
        "kfold = KFold(3, True, 1)\r\n",
        "# enumerate splits\r\n",
        "for train, test in kfold.split(df):\r\n",
        "\tprint('train: %s, test: %s' % (train_df, test_df))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train:      principal component 1  principal component 2  ...  principal component 4  label\n",
            "361               0.180217               1.755048  ...              -0.844578      1\n",
            "262               0.317124              -1.887943  ...              -0.270243      1\n",
            "92               -1.398340               1.868493  ...              -0.334578      1\n",
            "479              -2.564691               1.981823  ...               0.145573      1\n",
            "496               0.767590              -0.787185  ...               0.010289      1\n",
            "..                     ...                    ...  ...                    ...    ...\n",
            "645              -0.330780              -2.372383  ...              -0.636352      1\n",
            "715              -0.027997               0.931396  ...              -1.234619      1\n",
            "72                1.000634              -1.218521  ...              -0.112985      0\n",
            "235              -1.563920               0.134973  ...               0.113586      0\n",
            "37                0.347815              -2.006603  ...              -0.264749      1\n",
            "\n",
            "[604 rows x 5 columns], test:      principal component 1  principal component 2  ...  principal component 4  label\n",
            "457              -1.503536              -0.845675  ...              -0.988356      1\n",
            "537              -0.401074              -1.990746  ...              -0.152742      1\n",
            "636               1.414190              -0.724466  ...               0.666051      1\n",
            "386               0.870550              -0.050470  ...               1.971443      1\n",
            "412              -0.302023              -0.433963  ...              -0.786093      1\n",
            "..                     ...                    ...  ...                    ...    ...\n",
            "670              -1.949066               1.347755  ...               0.258311      1\n",
            "738               1.000169              -1.059721  ...               0.140891      0\n",
            "245              -1.259179              -0.498452  ...               1.942519      1\n",
            "13               -0.329871               2.293496  ...              -0.872697      1\n",
            "459              -0.192000              -2.724420  ...               0.879333      0\n",
            "\n",
            "[152 rows x 5 columns]\n",
            "train:      principal component 1  principal component 2  ...  principal component 4  label\n",
            "361               0.180217               1.755048  ...              -0.844578      1\n",
            "262               0.317124              -1.887943  ...              -0.270243      1\n",
            "92               -1.398340               1.868493  ...              -0.334578      1\n",
            "479              -2.564691               1.981823  ...               0.145573      1\n",
            "496               0.767590              -0.787185  ...               0.010289      1\n",
            "..                     ...                    ...  ...                    ...    ...\n",
            "645              -0.330780              -2.372383  ...              -0.636352      1\n",
            "715              -0.027997               0.931396  ...              -1.234619      1\n",
            "72                1.000634              -1.218521  ...              -0.112985      0\n",
            "235              -1.563920               0.134973  ...               0.113586      0\n",
            "37                0.347815              -2.006603  ...              -0.264749      1\n",
            "\n",
            "[604 rows x 5 columns], test:      principal component 1  principal component 2  ...  principal component 4  label\n",
            "457              -1.503536              -0.845675  ...              -0.988356      1\n",
            "537              -0.401074              -1.990746  ...              -0.152742      1\n",
            "636               1.414190              -0.724466  ...               0.666051      1\n",
            "386               0.870550              -0.050470  ...               1.971443      1\n",
            "412              -0.302023              -0.433963  ...              -0.786093      1\n",
            "..                     ...                    ...  ...                    ...    ...\n",
            "670              -1.949066               1.347755  ...               0.258311      1\n",
            "738               1.000169              -1.059721  ...               0.140891      0\n",
            "245              -1.259179              -0.498452  ...               1.942519      1\n",
            "13               -0.329871               2.293496  ...              -0.872697      1\n",
            "459              -0.192000              -2.724420  ...               0.879333      0\n",
            "\n",
            "[152 rows x 5 columns]\n",
            "train:      principal component 1  principal component 2  ...  principal component 4  label\n",
            "361               0.180217               1.755048  ...              -0.844578      1\n",
            "262               0.317124              -1.887943  ...              -0.270243      1\n",
            "92               -1.398340               1.868493  ...              -0.334578      1\n",
            "479              -2.564691               1.981823  ...               0.145573      1\n",
            "496               0.767590              -0.787185  ...               0.010289      1\n",
            "..                     ...                    ...  ...                    ...    ...\n",
            "645              -0.330780              -2.372383  ...              -0.636352      1\n",
            "715              -0.027997               0.931396  ...              -1.234619      1\n",
            "72                1.000634              -1.218521  ...              -0.112985      0\n",
            "235              -1.563920               0.134973  ...               0.113586      0\n",
            "37                0.347815              -2.006603  ...              -0.264749      1\n",
            "\n",
            "[604 rows x 5 columns], test:      principal component 1  principal component 2  ...  principal component 4  label\n",
            "457              -1.503536              -0.845675  ...              -0.988356      1\n",
            "537              -0.401074              -1.990746  ...              -0.152742      1\n",
            "636               1.414190              -0.724466  ...               0.666051      1\n",
            "386               0.870550              -0.050470  ...               1.971443      1\n",
            "412              -0.302023              -0.433963  ...              -0.786093      1\n",
            "..                     ...                    ...  ...                    ...    ...\n",
            "670              -1.949066               1.347755  ...               0.258311      1\n",
            "738               1.000169              -1.059721  ...               0.140891      0\n",
            "245              -1.259179              -0.498452  ...               1.942519      1\n",
            "13               -0.329871               2.293496  ...              -0.872697      1\n",
            "459              -0.192000              -2.724420  ...               0.879333      0\n",
            "\n",
            "[152 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jxbl7LTnxi9",
        "outputId": "37d2c134-95f8-44fe-b2fc-70d689913970"
      },
      "source": [
        "# bagging classifier\r\n",
        "from sklearn.ensemble import BaggingClassifier\r\n",
        "from sklearn.svm import SVC\r\n",
        "bagging = BaggingClassifier(base_estimator=SVC(), n_estimators=10, random_state=0)\r\n",
        "\r\n",
        "acc_score = []\r\n",
        "for train_index , test_index in kfold.split(df):\r\n",
        "    X_train , X_test = df.iloc[train_index,:],df.iloc[test_index,:]\r\n",
        "    Y_train , Y_test = labels[train_index] , labels[test_index]\r\n",
        "    bagging.fit(X_train,Y_train)\r\n",
        "    pred_values = bagging.predict(X_test)\r\n",
        "     \r\n",
        "    acc = accuracy_score(pred_values , Y_test)\r\n",
        "    acc_score.append(acc)\r\n",
        "\r\n",
        "print(\"Our accuracy scores:\\n\",acc_score)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our accuracy scores:\n",
            " [1.0, 1.0, 0.9880952380952381]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "yqHiOhkko0O-",
        "outputId": "4d448aac-eaac-4527-b3a6-8136dbd54dce"
      },
      "source": [
        "# Confusion Matrix\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, pred_values)\r\n",
        "\r\n",
        "# Visualizing the Confusion Matrix\r\n",
        "class_names = [0,1] # Our diagnosis categories\r\n",
        "\r\n",
        "fig, ax = plt.subplots()\r\n",
        "# Setting up and visualizing the plot \r\n",
        "tick_marks = np.arange(len(class_names)) \r\n",
        "plt.xticks(tick_marks, class_names)\r\n",
        "plt.yticks(tick_marks, class_names)\r\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g') # Creating heatmap\r\n",
        "ax.xaxis.set_label_position(\"top\")\r\n",
        "plt.tight_layout()\r\n",
        "plt.title('Confusion matrix', y = 1.1)\r\n",
        "plt.ylabel('Actual diagnosis')\r\n",
        "plt.xlabel('Predicted diagnosis')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 257.44, 'Predicted diagnosis')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kBcyP9mywz9"
      },
      "source": [
        "This ensemble method has outperformed any other single method, even achieving perfect accuracy. "
      ]
    }
  ]
}